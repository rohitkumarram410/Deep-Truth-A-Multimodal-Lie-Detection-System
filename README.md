# Deep Truth: A Multimodal Lie Detection System

## ğŸ“˜ Project Overview

**"Real-Time Lie Detection via Micro-Expression and Voice Analysis"** is an AI-driven system designed to identify deceptive behavior through involuntary facial cues and vocal patterns. Micro-expressionsâ€”fleeting, subconscious muscle movementsâ€”can reveal hidden emotions often suppressed during deception. By integrating **Computer Vision**, **Machine Learning**, and **Speech Analysis**, this project provides a **non-invasive, real-time lie detection framework** that mimics human intuition with computational precision.

---

## ğŸ§  Key Features

- âš¡ **Real-Time Detection** of lies through facial expressions and speech
- ğŸ‘ï¸ **Facial Landmark Tracking** using MediaPipe for micro-movement analysis
- ğŸ™ï¸ **Voice Input Processing** for natural speech-based detection
- ğŸ“ˆ **Multimodal Classification** using deep learning models
- ğŸ§° **Preprocessing & Augmentation** pipeline for enhanced robustness
- ğŸ–¼ï¸ **Live Visualization** with labeled outputs (Lie/Truth + probability score)
- ğŸ”’ **Optional Face Snapshot Saving** with prediction logging

---

## ğŸ”§ Technologies & Tools

| Tool            | Purpose                                           |
|------------------|---------------------------------------------------|
| ğŸ Python         | Core programming language                         |
| ğŸ“· OpenCV         | Frame capture, face detection, and display       |
| ğŸ¯ MediaPipe      | Real-time facial landmark detection              |
| ğŸ—£ï¸ SpeechRecognition | Audio capture and speech-to-text conversion     |
| ğŸ¤– TensorFlow / PyTorch | Deep learning model training & inference     |
| ğŸŒ² Scikit-learn (Random Forest) | Baseline ML classifier              |
| ğŸ§µ Threading       | Asynchronous processing for real-time execution |

---

## ğŸ§ª How It Works

1. **Capture:** Video input is split into frames; face and landmarks are detected in real-time.
2. **Voice Input:** Speech is transcribed and analyzed during response.
3. **Feature Extraction:** Facial tension zones and vocal patterns are extracted.
4. **Classification:** The model classifies the input as **Lie** or **Truth**.
5. **Output:** Prediction is shown live, with optional face snapshot logging.

---

## ğŸŒ Real-World Applications

- ğŸ‘®â€â™‚ï¸ **Law Enforcement & Interrogation Analysis**
- ğŸ§  **Psychological & Behavioral Research**
- ğŸ’¼ **Job Interviews & Candidate Screening**
- ğŸ¤– **Emotion-Aware Human-Computer Interaction (HCI)**
- ğŸ›¡ï¸ **Border Security & Surveillance Intelligence**

---

## ğŸš€ Why This Matters

This project explores the intersection of AI and human psychology, offering a non-intrusive, scalable, and real-time approach to understanding human behavior through facial expressions. As smart systems become more human-aware, tools like these can redefine the way we interpret and respond to emotions in both digital and real-world settings.
